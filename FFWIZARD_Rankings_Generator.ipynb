{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFWIZARD.COM --- Draft Rankings Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import StringIO\n",
    "from sklearn import preprocessing\n",
    "from pandas import ExcelWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in ADP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_adp(ppr, teams):\n",
    "    \n",
    "    # pull adp url with ppr and teams params\n",
    "    url = 'https://fantasyfootballcalculator.com/adp_csv.php?format=' + ppr + '&teams=' + teams\n",
    "    page_raw = requests.get(url)\n",
    "    page_txt = str(page_raw.text)[:]\n",
    "\n",
    "    # pull in text starting where data is...\n",
    "    bye_char = page_txt.find(\"Bye\") + 4\n",
    "    page_data = page_txt[bye_char:]\n",
    "\n",
    "    # set up list and initialize dataframe\n",
    "    split_txt = page_data.splitlines()\n",
    "    df = pd.DataFrame(split_txt[0].split(\",\")).T\n",
    "\n",
    "    # iterate through lists and form dataframe\n",
    "    num_players = len(split_txt) - 1\n",
    "    for i in range(1, num_players):\n",
    "        new_row = pd.DataFrame(split_txt[i].split(\",\")).T\n",
    "        df = pd.concat([df, new_row], axis=0)\n",
    "\n",
    "    # format dataframe and include relevant info\n",
    "    df = df.reset_index().iloc[:, [2, 3, 4, 5, 7]]\n",
    "    df.columns = [\"Rank\", \"Name\", \"Pos\", \"Team\", \"Bye\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NFL Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load nfl schedule data (manually entered)\n",
    "def load_schedule():\n",
    "    nfl_sched = pd.read_csv('2017_NFL_Schedule.csv')\n",
    "    return nfl_sched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defense Strength Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_defense_strength(bye_adjustor):\n",
    "\n",
    "    # load nfl defense strength data (manually entered)\n",
    "    def_strength_data = pd.read_csv('Defense_Strength.csv')\n",
    "\n",
    "    #remove 'source' row\n",
    "    def_strength = def_strength_data.iloc[:-1, :]\n",
    "\n",
    "    # make weights based on quality of rankings\n",
    "    def_rank_weights = pd.Series([0.1, 0.5, 0.3, 0.1])\n",
    "    \n",
    "    # create weighted defense rankings array based on weights\n",
    "    num_teams = len(def_strength)\n",
    "    weighted_def_rank = np.empty(num_teams, dtype=float)\n",
    "    \n",
    "    for i in range(0, num_teams):\n",
    "        weighted_def_rank[i] = def_rank_weights[0] * float(def_strength.iloc[i, 1]) + def_rank_weights[1] * float(def_strength.iloc[i, 2]) + def_rank_weights[2] * float(def_strength.iloc[i, 3]) + def_rank_weights[3] * float(def_strength.iloc[i, 4])\n",
    "    weighted_def_rank_df = pd.Series(weighted_def_rank)\n",
    "\n",
    "    # scale weighted defense rankings to have mean of 0 and std dev of 1\n",
    "    def_rank_scaled = pd.Series(preprocessing.scale(weighted_def_rank_df))\n",
    "\n",
    "    # create table with team and score info\n",
    "    defense_score_no_bye = pd.concat([def_strength.loc[:, \"Team\"], weighted_def_rank_df, def_rank_scaled], axis=1)\n",
    "    col_names = [\"Team\", \"Def_Score\", \"Def_Score_Scaled\"]\n",
    "    defense_score_no_bye.columns = col_names\n",
    "\n",
    "    # add value for bye weeks in table\n",
    "    bye_array = pd.DataFrame([\"BYE\", \"XXX\", bye_adjustor]).T\n",
    "    bye_array.columns = col_names\n",
    "    defense_score = pd.concat([defense_score_no_bye, bye_array])\n",
    "\n",
    "    defense_score = defense_score.reset_index(drop=True)\n",
    "    \n",
    "    return defense_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offense Strength Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_offense_strength(bye_adjustor):\n",
    "\n",
    "    # load nfl offense strength data (manually entered)\n",
    "    off_strength_data = pd.read_csv('Offense_Strength.csv')\n",
    "\n",
    "    # remove 'source' row\n",
    "    off_strength = off_strength_data.iloc[:-1, :]\n",
    "\n",
    "    # make weights based on quality of rankings\n",
    "    off_rank_weights = pd.Series([0.3, 0.1, 0.3, 0.25, 0.05])\n",
    "    \n",
    "    # create weighted offense rankings array based on weights\n",
    "    num_teams = len(off_strength)\n",
    "    weighted_off_rank = np.empty(num_teams, dtype=float)\n",
    "    \n",
    "    for i in range(0, num_teams):\n",
    "        weighted_off_rank[i] = off_rank_weights[0] * float(off_strength.iloc[i, 1]) + off_rank_weights[1] * float(off_strength.iloc[i, 2]) + off_rank_weights[2] * float(off_strength.iloc[i, 3]) + off_rank_weights[3] * float(off_strength.iloc[i, 4])  + off_rank_weights[4] * float(off_strength.iloc[i, 5])\n",
    "    weighted_off_rank_df = pd.Series(weighted_off_rank)\n",
    "\n",
    "    # scale weighted offense rankings to have mean of 0 and std dev of 1\n",
    "    off_rank_scaled = pd.Series(preprocessing.scale(weighted_off_rank_df))\n",
    "\n",
    "    # create table with team and score info\n",
    "    offense_score_no_bye = pd.concat([off_strength.loc[:, \"Team\"], weighted_off_rank_df, off_rank_scaled], axis=1)\n",
    "    col_names = [\"Team\", \"Off_Score\", \"Off_Score_Scaled\"]\n",
    "    offense_score_no_bye.columns = col_names\n",
    "\n",
    "    # add value for bye weeks in table\n",
    "    bye_array = pd.DataFrame([\"BYE\", \"XXX\", bye_adjustor]).T\n",
    "    bye_array.columns = col_names\n",
    "    offense_score = pd.concat([offense_score_no_bye, bye_array])\n",
    "\n",
    "    offense_score = offense_score.reset_index(drop=True)\n",
    "    \n",
    "    return offense_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Schedule to ADP Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sched_per_player(df, nfl_sched):\n",
    "\n",
    "    # create blank table to store opponent names\n",
    "    opponents = np.empty([len(df), 17], dtype=\"S3\")\n",
    "\n",
    "    # iterate through each player and every week\n",
    "    for i in range(0, len(df)):\n",
    "        for j in range(0, len(nfl_sched)):\n",
    "\n",
    "            # set weekly schedule for each player based on team\n",
    "            if (df.loc[i, \"Team\"] == nfl_sched.loc[j, \"Team\"]):\n",
    "                opponents[i, :] = nfl_sched.iloc[j, 1:]\n",
    "    \n",
    "    # put schedule into organized dataframe\n",
    "    opponents_df = pd.DataFrame(opponents)\n",
    "    opponents_df.columns = [\"Opp_1\", \"Opp_2\", \"Opp_3\", \"Opp_4\", \"Opp_5\", \"Opp_6\", \"Opp_7\", \"Opp_8\", \"Opp_9\", \"Opp_10\", \"Opp_11\", \"Opp_12\", \"Opp_13\", \"Opp_14\", \"Opp_15\", \"Opp_16\", \"Opp_17\"]\n",
    "    \n",
    "    return opponents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Matchup Strength Table by Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weekly_matchup_strength(df, opponents_df, offense_score, defense_score):\n",
    "\n",
    "    # create empty dataframe to store matchup strengths by week\n",
    "    str_scores = np.empty([len(df), 17], dtype=float)\n",
    "\n",
    "    \n",
    "    # iterate through each players matchup by week\n",
    "    for i in range(0, opponents_df.shape[0]):\n",
    "        for j in range(0, opponents_df.shape[1]):\n",
    "\n",
    "            # if the position is a defense, set matchup strengths based opponents' offense...\n",
    "            if (df.loc[i, \"Pos\"] == \"DEF\"):\n",
    "                for k in range(0, len(offense_score)):\n",
    "                    if (opponents_df.iloc[i, j] == offense_score.loc[k, \"Team\"]):\n",
    "                        str_scores[i, j] = offense_score.loc[k, \"Off_Score_Scaled\"]\n",
    "\n",
    "            # ...otherwise, player is an offensive player, so set matchup strengths based on opponents' defense\n",
    "            else:\n",
    "                for l in range(0, len(defense_score)):\n",
    "                    if (opponents_df.iloc[i, j] == defense_score.loc[l, \"Team\"]):\n",
    "                        str_scores[i, j] = defense_score.loc[l, \"Def_Score_Scaled\"]\n",
    "\n",
    "    # put matchup strengths into organized dataframe\n",
    "    str_scores_unwgt = pd.DataFrame(str_scores)\n",
    "    str_scores_unwgt.columns = [\"Str_1\", \"Str_2\", \"Str_3\", \"Str_4\", \"Str_5\", \"Str_6\", \"Str_7\", \"Str_8\", \"Str_9\", \"Str_10\", \"Str_11\", \"Str_12\", \"Str_13\", \"Str_14\", \"Str_15\", \"Str_16\", \"Str_17\"]\n",
    "\n",
    "    return str_scores_unwgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Week-by-Week Weightings Array (Higher for Early Weeks and Playoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def week_weight_table(first_playoff_week, last_playoff_week, playoff_weight, week_weights_thru_wk13_qb_wr_k_def, week_weights_thru_wk13_rb_te):\n",
    "    \n",
    "    # make blank list to store possible playoff weeks, 14 to 17\n",
    "    playoff_weeks_qb_wr_k_def = []\n",
    "    playoff_weeks_rb_te = []\n",
    "\n",
    "    # week 14\n",
    "    if (first_playoff_week == 14):\n",
    "        playoff_weeks_qb_wr_k_def.append(playoff_weight)\n",
    "        playoff_weeks_rb_te.append(playoff_weight)\n",
    "    else:\n",
    "        playoff_weeks_qb_wr_k_def.append(week_weights_thru_wk13_qb_wr_k_def[12])\n",
    "        playoff_weeks_rb_te.append(week_weights_thru_wk13_rb_te[12])\n",
    "\n",
    "    # week 15\n",
    "    if (first_playoff_week >= 14):\n",
    "        playoff_weeks_qb_wr_k_def.append(playoff_weight)\n",
    "        playoff_weeks_rb_te.append(playoff_weight)\n",
    "    else:\n",
    "        playoff_weeks_qb_wr_k_def.append(week_weights_thru_wk13_qb_wr_k_def[12])\n",
    "        playoff_weeks_rb_te.append(week_weights_thru_wk13_rb_te[12])\n",
    "\n",
    "    # week 16\n",
    "    if (last_playoff_week >= 16):\n",
    "        playoff_weeks_qb_wr_k_def.append(playoff_weight)\n",
    "        playoff_weeks_rb_te.append(playoff_weight)\n",
    "    else:\n",
    "        playoff_weeks_qb_wr_k_def.append(0)\n",
    "        playoff_weeks_rb_te.append(0)\n",
    "\n",
    "    # week 17\n",
    "    if (last_playoff_week == 17):\n",
    "        playoff_weeks_qb_wr_k_def.append(playoff_weight)\n",
    "        playoff_weeks_rb_te.append(playoff_weight)\n",
    "    else:\n",
    "        playoff_weeks_qb_wr_k_def.append(0)\n",
    "        playoff_weeks_rb_te.append(0)\n",
    "\n",
    "    # combine weeks 1 to 13 with playoff weeks\n",
    "    week_weights_qb_wr_k_def = week_weights_thru_wk13_qb_wr_k_def + playoff_weeks_qb_wr_k_def\n",
    "    week_weights_rb_te = week_weights_thru_wk13_rb_te + playoff_weeks_rb_te\n",
    "    \n",
    "    return week_weights_qb_wr_k_def, week_weights_rb_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Week-by-Week Weights to Player Weekly Matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_str_scores(df, str_scores_unwgt, week_weights_qb_wr_k_def, week_weights_rb_te, first_playoff_week, last_playoff_week, low_rank_adjustor):\n",
    "\n",
    "    # pull in adp ranks as floats\n",
    "    df_rank = df.loc[:, \"Rank\"]\n",
    "    df_rank = map(float, df_rank)\n",
    "    df_rank_array = np.array(df_rank)\n",
    "    \n",
    "    # make copy of weekly matchup strengths for each player\n",
    "    str_scores_wgt = str_scores_unwgt.copy()\n",
    "\n",
    "    # iterate through each row and multiply matchup strengths by weekly weights\n",
    "    for i in range(0, len(str_scores_unwgt)):\n",
    "        \n",
    "        # if player position is RB or TE, apply weight curve reflecting greater chance of injury\n",
    "        if (df.loc[i, \"Pos\"] == \"RB\") or (df.loc[i, \"Pos\"] == \"TE\"):\n",
    "            wgt_str_scores = np.multiply(np.asarray(str_scores_unwgt.iloc[i, :]), np.asarray(week_weights_rb_te))\n",
    "            str_scores_wgt.iloc[i, :] = wgt_str_scores\n",
    "        \n",
    "        # otherwise apply differenct curve\n",
    "        else:\n",
    "            wgt_str_scores = np.multiply(np.asarray(str_scores_unwgt.iloc[i, :]), np.asarray(week_weights_qb_wr_k_def))\n",
    "            str_scores_wgt.iloc[i, :] = wgt_str_scores\n",
    "            \n",
    "                    \n",
    "        for j in range(first_playoff_week-1, last_playoff_week-1):\n",
    "            str_scores_wgt.iloc[i, j] = wgt_str_scores[j] * (1 - (df_rank_array[i] / low_rank_adjustor))\n",
    "\n",
    "    # put weighted matchup strengths into organized dataframe\n",
    "    str_scores_wgt.columns = [\"Wgt_1\", \"Wgt_2\", \"Wgt_3\", \"Wgt_4\", \"Wgt_5\", \"Wgt_6\", \"Wgt_7\", \"Wgt_8\", \"Wgt_9\", \"Wgt_10\", \"Wgt_11\", \"Wgt_12\", \"Wgt_13\", \"Wgt_14\", \"Wgt_15\", \"Wgt_16\", \"Wgt_17\"]\n",
    "    \n",
    "    return str_scores_wgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Strength Rating Per Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def player_weighted_sched_str(str_scores_wgt, str_scores_unwgt, overall_sos_weight):\n",
    "\n",
    "    # set up dummy strength array (weighted)\n",
    "    str_scores_sum_wgt = str_scores_wgt.iloc[:, 0]\n",
    "\n",
    "    # populate array with summed scores (weighted)\n",
    "    for i in range(0, len(str_scores_wgt)):\n",
    "        str_scores_sum_wgt[i] = str_scores_wgt.iloc[i, :].sum()\n",
    "\n",
    "    # set up dummy strength array (unweighted)\n",
    "    str_scores_sum_unwgt = str_scores_unwgt.iloc[:, 0]\n",
    "    \n",
    "    # populate array with summed scores (unweighted)\n",
    "    for j in range(0, len(str_scores_unwgt)):\n",
    "        str_scores_sum_unwgt[j] = str_scores_unwgt.iloc[j, :].sum()\n",
    "    \n",
    "    # scale weighted sos values (mean = 0, var = 1)\n",
    "    str_scores_scaled_wgt = pd.Series(preprocessing.scale(str_scores_sum_wgt))\n",
    "\n",
    "    # scale unweighted sos values (mean = 0, var = 1)\n",
    "    str_scores_scaled_unwgt = pd.Series(preprocessing.scale(str_scores_sum_unwgt))\n",
    "    \n",
    "    \n",
    "    # set up dummy array adjusting out strength of scedule (unscaled)\n",
    "    str_scores_unscaled = str_scores_unwgt.iloc[:, 0]\n",
    "    \n",
    "    # populate array with summed scores (unweighted)\n",
    "    for k in range(0, len(str_scores_unscaled)):\n",
    "        str_scores_unscaled[k] = str_scores_scaled_wgt[k] - (overall_sos_weight * str_scores_scaled_unwgt[k])\n",
    "    \n",
    "    # scale unweighted sos values (mean = 0, var = 1)\n",
    "    str_scores_scaled = pd.Series(preprocessing.scale(str_scores_unscaled))\n",
    "    \n",
    "    return str_scores_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Strength of Schedule Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ffwiz_score_scaled(df, str_scores_scaled, rank_adder, matchup_strength_scaler, def_rank_penalty, k_rank_penalty):\n",
    "\n",
    "    # pull in adp ranks and SoS scores\n",
    "    df_rank = df.loc[:, \"Rank\"]\n",
    "    df_rank = map(float, df_rank)\n",
    "    df_rank_array = np.array(df_rank)\n",
    "    \n",
    "    # initialize adjusted rankings\n",
    "    df_rank_adj = df_rank_array.copy()\n",
    "\n",
    "    # iterate through the players and add penalty to DEF and K\n",
    "    for i in range(0, len(df)):\n",
    "        if df.loc[i, \"Pos\"] == \"DEF\":\n",
    "            df_rank_adj[i] = df_rank_array[i] + rank_adder + def_rank_penalty\n",
    "        elif df.loc[i, \"Pos\"] == \"PK\":\n",
    "            df_rank_adj[i] = df_rank_array[i] + rank_adder + k_rank_penalty\n",
    "        else:\n",
    "            df_rank_adj[i] = df_rank_array[i] + rank_adder\n",
    "\n",
    "    # adjust adp ranks and SoS scores (to move players up/down appropriate slots)\n",
    "    str_scores_scaled_adj = matchup_strength_scaler - str_scores_scaled\n",
    "\n",
    "    # multiply adjusted adp ranks and SoS scores\n",
    "    df_rank_adj = np.multiply(df_rank_adj, str_scores_scaled_adj)\n",
    "    ffwiz_score = np.asarray(df_rank_adj)\n",
    "    \n",
    "    return(ffwiz_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall FFWIZ Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ffwiz_overall_rankings(df, ffwiz_score):\n",
    "\n",
    "    # sort strength of schedule scores and put into dataframe\n",
    "    ffwiz_score_df = pd.DataFrame(ffwiz_score, columns=[\"FFWIZ_Score\"])\n",
    "    ffwiz_score_sort = ffwiz_score_df.sort_values(by='FFWIZ_Score')\n",
    "    ffwiz_sort_idx = ffwiz_score_sort.index.values + 1\n",
    "    ffwiz_sort_df = pd.DataFrame(ffwiz_sort_idx, columns=[\"FFWIZ\"])\n",
    "\n",
    "    # get ffwiz overall rankings using above dataframe\n",
    "    ff_sort = ffwiz_sort_df.sort_values(by='FFWIZ')\n",
    "    ff_sort_idx = ff_sort.index.values + 1\n",
    "    ff_sort_df = pd.DataFrame(ff_sort_idx, columns=[\"FFWIZ\"])\n",
    "\n",
    "    # make dataframe for adp rankings\n",
    "    adp_rank_df = pd.DataFrame(range(1, len(ffwiz_sort_idx)+1), columns=[\"ADP\"])\n",
    "    adp_values = np.array(range(1, len(ffwiz_sort_idx)+1))\n",
    "\n",
    "    # calculate difference in ffwiz vs. adp rankings\n",
    "    diff = pd.DataFrame(np.asarray(adp_rank_df) - np.asarray(ff_sort_df), columns=[\"DIFF\"])\n",
    "\n",
    "    # make master dataframe and sort\n",
    "    ffwiz_df = pd.concat([ff_sort_df, adp_rank_df, diff, df.iloc[:, 1:]], axis=1)\n",
    "    ffwiz_df_overall = ffwiz_df.sort_values(by='FFWIZ').reset_index(drop=True)\n",
    "\n",
    "    return ffwiz_df_overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional FFWIZ Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ffwiz_df_positional(ffwiz_df_overall):\n",
    "\n",
    "    # FFWIZ\n",
    "    # sort by position and add FFWIZ and ADP positional rankings\n",
    "    ffwiz_df_no_pos_rank = ffwiz_df_overall.sort_values(by=['Pos', 'FFWIZ']).reset_index(drop=True)\n",
    "\n",
    "    # add positional ranks to FFWIZ rankings\n",
    "    ffwiz_pos_rank_list = [1] #initialize pos list\n",
    "    ffwiz_rank_counter = 1 #initialize positional rank to 1\n",
    "\n",
    "    # populate positional rank list\n",
    "    for k in range(1, len(ffwiz_df_overall)):\n",
    "\n",
    "        # reset positional rank value if \n",
    "        if (ffwiz_df_no_pos_rank.loc[k, 'Pos'] == ffwiz_df_no_pos_rank.loc[k-1, 'Pos']):\n",
    "            ffwiz_rank_counter = ffwiz_rank_counter + 1\n",
    "        else:\n",
    "            ffwiz_rank_counter = 1\n",
    "\n",
    "        ffwiz_pos_rank_list.append(ffwiz_rank_counter)\n",
    "\n",
    "    ffwiz_df_pos_rank = pd.DataFrame(ffwiz_pos_rank_list, columns=[\"FFWIZ_Pos_Rank\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # ADP\n",
    "    # sort by position and add and ADP positional rankings\n",
    "    adp_df_no_pos_rank = ffwiz_df_overall.sort_values(by=['Pos', 'ADP']).reset_index(drop=True)\n",
    "    adp_pos_rank_table = pd.concat([adp_df_no_pos_rank, ffwiz_df_pos_rank], axis=1)\n",
    "    adp_pos_sort_ffwiz = adp_pos_rank_table.sort_values(by=['Pos', 'FFWIZ']).reset_index(drop=True)\n",
    "    adp_pos_rank_array = np.asarray(adp_pos_sort_ffwiz.loc[:, 'FFWIZ_Pos_Rank'])\n",
    "    adp_pos_rank = pd.DataFrame(adp_pos_rank_array, columns=[\"ADP_Pos_Rank\"])\n",
    "\n",
    "    # DIFF & CONCATENATE\n",
    "    ffwiz_pos_rank_array = np.asarray(ffwiz_df_pos_rank.loc[:, 'FFWIZ_Pos_Rank'])\n",
    "    pos_diff = pd.DataFrame(adp_pos_rank_array - ffwiz_pos_rank_array, columns=[\"Pos_DIFF\"])\n",
    "    ffwiz_df_pos = pd.concat([ffwiz_df_pos_rank, adp_pos_rank, pos_diff, ffwiz_df_no_pos_rank], axis=1)\n",
    "\n",
    "    return ffwiz_df_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Overall and Positional Rankings (Fxns of Fxns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_ffwiz_data(teams, ppr, first_playoff_week, last_playoff_week, week_weights_thru_wk13_qb_wr_k_def, week_weights_thru_wk13_rb_te, playoff_weight, rank_adder, matchup_strength_scaler, overall_sos_weight, def_rank_penalty, k_rank_penalty):\n",
    "    \n",
    "    # pull in adp data and schedule based on PPR selection and # of teams in league (FROM USER)\n",
    "    df = pull_adp(ppr, teams)\n",
    "    nfl_sched = load_schedule()\n",
    "\n",
    "    # make dataframe with each players opponents\n",
    "    opponents_df = sched_per_player(df, nfl_sched)\n",
    "\n",
    "    # calculate defense and offense strength scores\n",
    "    defense_score = calc_defense_strength(bye_adjustor)\n",
    "    offense_score = calc_offense_strength(bye_adjustor)\n",
    "\n",
    "    # map matchup strengths for each player for every week\n",
    "    str_scores_unwgt = weekly_matchup_strength(df, opponents_df, offense_score, defense_score)\n",
    "\n",
    "    # generate weekly weights based on inputted playoff weeks (FROM USER)\n",
    "    week_weights_qb_wr_k_def, week_weights_rb_te = week_weight_table(first_playoff_week, last_playoff_week, playoff_weight, week_weights_thru_wk13_qb_wr_k_def, week_weights_thru_wk13_rb_te)\n",
    "\n",
    "    # weight each weekly matchup for each player\n",
    "    str_scores_wgt = weight_str_scores(df, str_scores_unwgt, week_weights_qb_wr_k_def, week_weights_rb_te, first_playoff_week, last_playoff_week, low_rank_adjustor)\n",
    "\n",
    "    # get ONE value per player by summing all weighted weekly matchup scores and SCALING so mean=0, var=1\n",
    "    str_scores_scaled = player_weighted_sched_str(str_scores_wgt, str_scores_unwgt, overall_sos_weight)\n",
    "    \n",
    "    # get ffwiz absolute score rankings based on further adjusting of differentials\n",
    "    ffwiz_score = ffwiz_score_scaled(df, str_scores_scaled, rank_adder, matchup_strength_scaler, def_rank_penalty, k_rank_penalty)\n",
    "\n",
    "    # get overall ffwiz rankings\n",
    "    ffwiz_df_overall = ffwiz_overall_rankings(df, ffwiz_score)\n",
    "\n",
    "    # get positional ffwiz rankings\n",
    "    ffwiz_df_pos = ffwiz_df_positional(ffwiz_df_overall)\n",
    "    \n",
    "    return ffwiz_df_overall, ffwiz_df_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make List of Dataframe Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataframes_lists(teams_array, ppr_array, first_playoff_week_array, last_playoff_week_array):\n",
    "\n",
    "    ffwiz_overall_list = []\n",
    "    ffwiz_pos_list = []\n",
    "    tab_name_list = []\n",
    "\n",
    "    # iterate through all user inputs\n",
    "    for i in range(0, len(teams_array)):\n",
    "        for j in range(0, len(ppr_array)):\n",
    "            for k in range(0, len(first_playoff_week_array)):\n",
    "                for l in range(0, len(last_playoff_week_array)):\n",
    "                    \n",
    "                    #generate rankings\n",
    "                    ffwiz_df_overall, ffwiz_df_pos = generate_ffwiz_data(teams_array[i], ppr_array[j], first_playoff_week_array[k], last_playoff_week_array[l], week_weights_thru_wk13_qb_wr_k_def, week_weights_thru_wk13_rb_te, playoff_weight, rank_adder, matchup_strength_scaler, overall_sos_weight, def_rank_penalty, k_rank_penalty)\n",
    "\n",
    "                    ffwiz_overall_list.append(ffwiz_df_overall)\n",
    "                    ffwiz_pos_list.append(ffwiz_df_pos)\n",
    "                    \n",
    "                    #make list of tabnames for excel file\n",
    "                    tab_name =  teams_array[i] + '_' + str(j) + '_' + str(first_playoff_week_array[k]) + '_' + str(last_playoff_week_array[l])\n",
    "                    tab_name_list.append(tab_name)\n",
    "    \n",
    "                    # print to keep track of progress\n",
    "                    print(tab_name)\n",
    "                    print(str(datetime.now()))\n",
    "    \n",
    "    return ffwiz_overall_list, ffwiz_pos_list, tab_name_list       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Excel File with Overall and Positional Rankings Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_excel(ffwiz_overall_list, ffwiz_pos_list, tab_name_list):\n",
    "\n",
    "    # make writer object for Excel file\n",
    "    writer = pd.ExcelWriter('./rankings_data_files/rankings_data_master.xlsx', engine='xlsxwriter')\n",
    "    \n",
    "    # iterate through every input permutation\n",
    "    for i in range(0, len(tab_name_list)):\n",
    "        \n",
    "        # generate breakpoints of positions in order to split tabs by position\n",
    "        idx_list = []\n",
    "        for j in range(0, len(ffwiz_pos_list[i])):\n",
    "                if ffwiz_pos_list[i].loc[j, 'FFWIZ_Pos_Rank'] == 1:\n",
    "                    idx_list.append(j)\n",
    "        idx_list.append(j) #add END value for indexing\n",
    "\n",
    "        # create positional dataframes based on breakpoints above\n",
    "        ffwiz_def = ffwiz_pos_list[i].iloc[idx_list[0]:idx_list[1]]\n",
    "        ffwiz_k = ffwiz_pos_list[i].iloc[idx_list[1]:idx_list[2]]\n",
    "        ffwiz_qb = ffwiz_pos_list[i].iloc[idx_list[2]:idx_list[3]]\n",
    "        ffwiz_rb = ffwiz_pos_list[i].iloc[idx_list[3]:idx_list[4]]\n",
    "        ffwiz_te = ffwiz_pos_list[i].iloc[idx_list[4]:idx_list[5]]\n",
    "        ffwiz_wr = ffwiz_pos_list[i].iloc[idx_list[5]:idx_list[6]]\n",
    "\n",
    "        # create tabs for overall and each position in Excel file\n",
    "        ffwiz_overall_list[i].to_excel(writer, sheet_name=tab_name_list[i]+'_ALL')\n",
    "        ffwiz_def.to_excel(writer, sheet_name=tab_name_list[i]+'_DEF')\n",
    "        ffwiz_k.to_excel(writer, sheet_name=tab_name_list[i]+'_K')\n",
    "        ffwiz_qb.to_excel(writer, sheet_name=tab_name_list[i]+'_QB')\n",
    "        ffwiz_rb.to_excel(writer, sheet_name=tab_name_list[i]+'_RB')\n",
    "        ffwiz_te.to_excel(writer, sheet_name=tab_name_list[i]+'_TE')\n",
    "        ffwiz_wr.to_excel(writer, sheet_name=tab_name_list[i]+'_WR')\n",
    "\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GLOBAL INPUTS (constant regardless of user input)\n",
    "week_weights_thru_wk13_qb_wr_k_def = [1,0.98,0.96,0.94,0.92,0.90,0.88,0.86,0.84,0.82,0.80,0.78,0.76]  #based on games started analysis & trade assumptions\n",
    "week_weights_thru_wk13_rb_te = [1,0.96,0.92,0.88,0.84,0.80,0.76,0.72,0.68,0.64,0.60,0.56,0.52]  #based on games started analysis & trade assumptions\n",
    "playoff_weight = 1.2           # was 1.25\n",
    "rank_adder = 100               # was 100\n",
    "matchup_strength_scaler = 45   # was 50\n",
    "overall_sos_weight = 0.5       # was 0.5\n",
    "low_rank_adjustor = 350        # to adjust-out the importance of playoff weeks for lower-tier players\n",
    "k_rank_penalty = 5\n",
    "def_rank_penalty = 2\n",
    "bye_adjustor = -2.5\n",
    "\n",
    "\n",
    "# USER INPUTS\n",
    "teams_array = ['8', '10', '12', '14']\n",
    "ppr_array = ['standard', 'ppr']\n",
    "first_playoff_week_array = [14, 15]\n",
    "last_playoff_week_array = [15, 16, 17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Rankings and Make Excel Output (Run Functions!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ffwiz_overall_list, ffwiz_pos_list, tab_name_list = make_dataframes_lists(teams_array, ppr_array, first_playoff_week_array, last_playoff_week_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8_0_14_15\n",
      "2017-07-20 15:32:44.994000\n",
      "8_0_14_16\n",
      "2017-07-20 15:32:45.106000\n",
      "8_0_14_17\n",
      "2017-07-20 15:32:45.231000\n",
      "8_0_15_15\n",
      "2017-07-20 15:32:45.336000\n",
      "8_0_15_16\n",
      "2017-07-20 15:32:45.473000\n",
      "8_0_15_17\n",
      "2017-07-20 15:32:45.633000\n",
      "8_1_14_15\n",
      "2017-07-20 15:32:45.734000\n",
      "8_1_14_16\n",
      "2017-07-20 15:32:45.850000\n",
      "8_1_14_17\n",
      "2017-07-20 15:32:45.962000\n",
      "8_1_15_15\n",
      "2017-07-20 15:32:46.068000\n",
      "8_1_15_16\n",
      "2017-07-20 15:32:46.177000\n",
      "8_1_15_17\n",
      "2017-07-20 15:32:46.286000\n",
      "10_0_14_15\n",
      "2017-07-20 15:32:46.457000\n",
      "10_0_14_16\n",
      "2017-07-20 15:32:46.642000\n",
      "10_0_14_17\n",
      "2017-07-20 15:32:46.778000\n",
      "10_0_15_15\n",
      "2017-07-20 15:32:46.903000\n",
      "10_0_15_16\n",
      "2017-07-20 15:32:47.027000\n",
      "10_0_15_17\n",
      "2017-07-20 15:32:47.168000\n",
      "10_1_14_15\n",
      "2017-07-20 15:32:47.311000\n",
      "10_1_14_16\n",
      "2017-07-20 15:32:47.577000\n",
      "10_1_14_17\n",
      "2017-07-20 15:32:47.754000\n",
      "10_1_15_15\n",
      "2017-07-20 15:32:47.914000\n",
      "10_1_15_16\n",
      "2017-07-20 15:32:48.063000\n",
      "10_1_15_17\n",
      "2017-07-20 15:32:48.211000\n",
      "12_0_14_15\n",
      "2017-07-20 15:32:48.335000\n",
      "12_0_14_16\n",
      "2017-07-20 15:32:48.515000\n",
      "12_0_14_17\n",
      "2017-07-20 15:32:48.696000\n",
      "12_0_15_15\n",
      "2017-07-20 15:32:48.837000\n",
      "12_0_15_16\n",
      "2017-07-20 15:32:48.962000\n",
      "12_0_15_17\n",
      "2017-07-20 15:32:49.098000\n",
      "12_1_14_15\n",
      "2017-07-20 15:32:49.283000\n",
      "12_1_14_16\n",
      "2017-07-20 15:32:49.415000\n",
      "12_1_14_17\n",
      "2017-07-20 15:32:49.535000\n",
      "12_1_15_15\n",
      "2017-07-20 15:32:49.682000\n",
      "12_1_15_16\n",
      "2017-07-20 15:32:49.818000\n",
      "12_1_15_17\n",
      "2017-07-20 15:32:49.961000\n",
      "14_0_14_15\n",
      "2017-07-20 15:32:50.106000\n",
      "14_0_14_16\n",
      "2017-07-20 15:32:50.240000\n",
      "14_0_14_17\n",
      "2017-07-20 15:32:50.397000\n",
      "14_0_15_15\n",
      "2017-07-20 15:32:50.551000\n",
      "14_0_15_16\n",
      "2017-07-20 15:32:50.700000\n",
      "14_0_15_17\n",
      "2017-07-20 15:32:50.919000\n",
      "14_1_14_15\n",
      "2017-07-20 15:32:51.055000\n",
      "14_1_14_16\n",
      "2017-07-20 15:32:51.204000\n",
      "14_1_14_17\n",
      "2017-07-20 15:32:51.351000\n",
      "14_1_15_15\n",
      "2017-07-20 15:32:51.527000\n",
      "14_1_15_16\n",
      "2017-07-20 15:32:51.743000\n",
      "14_1_15_17\n",
      "2017-07-20 15:32:51.937000\n"
     ]
    }
   ],
   "source": [
    "make_excel(ffwiz_overall_list, ffwiz_pos_list, tab_name_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python2]",
   "language": "python",
   "name": "Python [python2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
